{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimsu/py36tf1x/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "from dataset.datasets import DataSet\n",
    "from attention.attention import model, model2, losses2, weight_variable, bias_variable, pretrain_losses\n",
    "from attention.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_image_coordinates(normalized_coordinate):\n",
    "    '''\n",
    "    Transform coordinate in [-1,1] to mnist\n",
    "    :param coordinate_tanh: vector in [-1,1] x [-1,1]\n",
    "    :return: vector in the corresponding mnist coordinate\n",
    "    '''\n",
    "    return np.round(((normalized_coordinate + 1) / 2.0) * img_sz)\n",
    "\n",
    "\n",
    "def variable_summaries(var, name):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "    with tf.name_scope('param_summaries'):\n",
    "        # mean = tf.reduce_mean(var)\n",
    "        # tf.summary.scalar('param_mean/' + name, mean)\n",
    "        # with tf.name_scope('param_stddev'):\n",
    "        #     stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        # tf.summary.scalar('param_sttdev/' + name, stddev)\n",
    "        # tf.summary.scalar('param_max/' + name, tf.reduce_max(var))\n",
    "        # tf.summary.scalar('param_min/' + name, tf.reduce_min(var))\n",
    "        tf.summary.histogram(name, var)\n",
    "\n",
    "\n",
    "def evaluate(datasets, sess, tensors):\n",
    "    '''\n",
    "    tensors : list  -->  [x, Y, t_acc, i_acc, m_acc, f_acc]\n",
    "    '''\n",
    "    data = datasets.test_data\n",
    "\n",
    "    _acc_element = [0] * (len(tensors) - 2)\n",
    "\n",
    "    n_iter = min(10, data.n_data // batch_size)\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        images, labels = data.next_batch(batch_size)\n",
    "        if images.shape[0] != batch_size:\n",
    "            images, labels = dataset.train_data.next_batch(batch_size)\n",
    "        if images[0].shape[0] != img_sz or images[0].shape[1] != img_sz:\n",
    "            images = np.array([cv2.resize(img, (img_sz, img_sz)) for img in images])\n",
    "        accs = sess.run(tensors[2:], feed_dict={tensors[0]: images, tensors[1]: labels})\n",
    "        for i in range(len(_acc_element)):\n",
    "            _acc_element[i] += accs[i]\n",
    "\n",
    "    for i in range(len(_acc_element)):\n",
    "        _acc_element[i] /= n_iter\n",
    "\n",
    "    acc_str = \" >> TOTAL ACCURACY: %.3f\" % _acc_element[0]\n",
    "    for i in range(len(_acc_element) - 1):\n",
    "        acc_str += \", %d th ACCURACY: %.3f\" % (i, _acc_element[i+1])\n",
    "\n",
    "    print(acc_str)\n",
    "    # print(\" >> TOTAL ACCURACY: %.3f, INITIAL ACCURACY: %.3f, MIDDLE ACCURACY: %.3f, FINAL ACCURACY: %.3f\" %\n",
    "    #       (_acc_t, _acc_i, _acc_m, _acc_f))\n",
    "\n",
    "\n",
    "def pretrain(c_recon, c_recon_cost, g_recon, g_recon_cost, total_recon_cost, train_op_r, step):\n",
    "    print(' Reconstruction Building.. (for pretraining)')\n",
    "\n",
    "    print(' Pre-Training Start..!')\n",
    "    start_time = time.time()\n",
    "    for step in range(step):\n",
    "        next_images, _ = dataset.train_data.next_batch(batch_size)\n",
    "        if next_images.shape[0] != batch_size:\n",
    "            next_images, _ = dataset.train_data.next_batch(batch_size)\n",
    "\n",
    "        feed_dict = {x: next_images}\n",
    "\n",
    "        fetches = [c_recon, c_recon_cost, g_recon, g_recon_cost, total_recon_cost, train_op_r]\n",
    "\n",
    "        c_r, c_r_cost, g_r, g_r_cost, t_cost, op_r = sess.run(fetches, feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            start_time = end_time\n",
    "            print(\n",
    "                'Step %d --> Reconstruction Pretraining: total_cost = %.5f, context_cost = %.3f, glimpse_cost = %.3f (%.3f sec)' % (\n",
    "                step, t_cost, c_r_cost, g_r_cost, duration))\n",
    "\n",
    "    print(' Pretraining Finish..!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Building model..\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x124a2df28>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x124a82588>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      " Defining loss function..\n",
      " Creating optimizer..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/2350 [00:00<01:02, 37.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Summarizing tensor variables and scalar to visualize by tensorboard..\n",
      " Loading datasets..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2350/2350 [01:29<00:00, 26.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Initializing session..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = {\n",
    "    'dataset': 'phd08',\n",
    "    'dataset_path': '/Users/kimsu/datasets/korean_image/phd08',\n",
    "    'width': img_sz,\n",
    "    'height': img_sz,\n",
    "    'sampling': True,\n",
    "    'n_sample': 20,\n",
    "    'train_set_ratio': 0.9\n",
    "}\n",
    "args['data_size'] = args['width'] * args['height']\n",
    "\n",
    "base_path = os.path.join(os.path.curdir, '20180701')\n",
    "summary_path = os.path.join(base_path, 'summary')\n",
    "save_path = os.path.join(base_path, 'save')\n",
    "image_log_path = os.path.join(base_path, 'image_log')\n",
    "\n",
    "if not os.path.exists(base_path):\n",
    "    os.mkdir(base_path)\n",
    "if not os.path.exists(summary_path):\n",
    "    os.mkdir(summary_path)\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "if not os.path.exists(image_log_path):\n",
    "    os.mkdir(image_log_path)\n",
    "\n",
    "draw = True\n",
    "\n",
    "# input / label image tensor\n",
    "x = tf.placeholder(tf.float32, [batch_size, img_sz, img_sz, 1], name='image')\n",
    "Y = tf.placeholder(tf.int64, shape=[batch_size, 3], name='label')\n",
    "\n",
    "# Weight and Bias variables\n",
    "w = {\n",
    "    # for context network\n",
    "    'wc1': weight_variable([3, 3, channels, 8], 'contextNet_weight_conv1'),\n",
    "    'wc2': weight_variable([3, 3, 8, 3], 'contextNet_weight_conv2'),\n",
    "    # 'wc3': weight_variable([3, 3, 64, 3], 'contextNet_weight_conv3'),\n",
    "    'wc_fc': weight_variable([img_len // 4 * 3, lstm_size * 2], 'contextNet_weight_fc'),\n",
    "\n",
    "    # 'wc1': tf.get_variable('contextNet_weight_conv1', [3, 3, channels, 16], tf.float32),\n",
    "    # 'wc2': tf.get_variable('contextNet_weight_conv2', [3, 3, 16, 64], tf.float32),\n",
    "    # 'wc3': tf.get_variable('contextNet_weight_conv3', [1, 1, 64, 3], tf.float32),\n",
    "    # 'wc_fc': tf.get_variable('contextNet_weight_fc', [img_len * 3, lstm_size * 2], tf.float32),\n",
    "    # for emission network\n",
    "    'we_bl': weight_variable([lstm_size, 1], 'emissionNet_weight_baseline'),\n",
    "    'we_h_nl': weight_variable([lstm_size, 1], 'emissionNet_weight_hidden_normalize_loc'),\n",
    "    # 'we_bl': tf.get_variable('emissionNet_weight_baseline', [lstm_size, 1], tf.float32),\n",
    "    # 'we_h_nl': tf.get_variable('emissionNet_weight_hidden_normalize_loc', [lstm_size, 2], tf.float32),\n",
    "    # for action network\n",
    "    'wai': weight_variable([lstm_size, n_initial_character], 'actionNet_weight_initial'),\n",
    "    'wam': weight_variable([lstm_size, n_middle_character], 'actionNet_weight_middle'),\n",
    "    'waf': weight_variable([lstm_size, n_final_character], 'actionNet_weight_final'),\n",
    "    # 'wai': tf.get_variable('actionNet_weight_initial', [lstm_size, n_initial_character], tf.float32),\n",
    "    # 'wam': tf.get_variable('actionNet_weight_middle', [lstm_size, n_middle_character], tf.float32),\n",
    "    # 'waf': tf.get_variable('actionNet_weight_final', [lstm_size, n_final_character], tf.float32),\n",
    "    # for glimpse network\n",
    "    # 'wg1': weight_variable([3, 3, channels, 8], 'glimpseNet_weight_conv1'),\n",
    "    'wg1': weight_variable([3, 3, g_depth, 8], 'glimpseNet_weight_conv1'),\n",
    "    'wg2': weight_variable([3, 3, 8, 3], 'glimpseNet_weight_conv2'),\n",
    "    # 'wg3': weight_variable([3, 3, 64, 3], 'glimpseNet_weight_conv3'),\n",
    "    'wg_fc': weight_variable([sensor_bandwidth * sensor_bandwidth * 3, lstm_size], 'glimpseNet_weight_fc'),\n",
    "    'wg_lh': weight_variable([2, lstm_size], 'glimpseNet_weight_loc2hidden'),\n",
    "    'wg_gh_gf': weight_variable([lstm_size, lstm_size], 'glimpseNet_weight_glimpse2feature'),\n",
    "    'wg_lh_gf': weight_variable([lstm_size, lstm_size], 'glimpseNet_weight_locHidden2feature'),\n",
    "\n",
    "    # 'wg1': tf.get_variable('glimpseNet_weight_conv1', [3, 3, channels, 16], tf.float32),\n",
    "    # 'wg2': tf.get_variable('glimpseNet_weight_conv2', [3, 3, 16, 64], tf.float32),\n",
    "    # 'wg3': tf.get_variable('glimpseNet_weight_conv3', [1, 1, 64, 3], tf.float32),\n",
    "    # 'wg_fc': tf.get_variable('glimpseNet_weight_fc', [sensor_bandwidth * sensor_bandwidth * 3, lstm_size], tf.float32),\n",
    "    # 'wg_lh': tf.get_variable('glimpseNet_weight_loc2hidden', [2, lstm_size], tf.float32),\n",
    "    # 'wg_gh_gf': tf.get_variable('glimpseNet_weight_glimpse2feature', [lstm_size, lstm_size], tf.float32),\n",
    "    # 'wg_lh_gf': tf.get_variable('glimpseNet_weight_locHidden2feature', [lstm_size, lstm_size], tf.float32),\n",
    "    # for core network\n",
    "    'wo': weight_variable([lstm_size, lstm_size], 'coreNet_weight_out'),\n",
    "    # 'wo': tf.get_variable('coreNet_weight_out', [lstm_size, lstm_size], tf.float32)\n",
    "}\n",
    "b = {\n",
    "    # for context network\n",
    "    'bc1': bias_variable([8], 'contextNet_bias_conv1'),\n",
    "    'bc2': bias_variable([3], 'contextNet_bias_conv2'),\n",
    "    # 'bc3': bias_variable([3], 'contextNet_bias_conv3'),\n",
    "    'bc_fc': bias_variable([lstm_size * 2], 'contextNet_bias_fc'),\n",
    "\n",
    "    # 'bc1': tf.get_variable('contextNet_bias_conv1', [16], tf.float32),\n",
    "    # 'bc2': tf.get_variable('contextNet_bias_conv2', [64], tf.float32),\n",
    "    # 'bc3': tf.get_variable('contextNet_bias_conv3', [3], tf.float32),\n",
    "    # 'bc_fc': tf.get_variable('contextNet_bias_fc', [lstm_size * 2], tf.float32),\n",
    "    # for emission network\n",
    "    'be_bl': bias_variable([1], 'emissionNet_bias_baseline'),\n",
    "    'be_h_nl': bias_variable([2], 'emissionNet_bias_hidden_normalize_loc'),\n",
    "    # 'be_bl': tf.get_variable('emissionNet_bias_baseline', [1], tf.float32),\n",
    "    # 'be_h_nl': tf.get_variable('emissionNet_bias_hidden_normalize_loc', [2], tf.float32),\n",
    "    # for action network\n",
    "    'bai': bias_variable([n_initial_character], 'actionNet_bias_initial'),\n",
    "    'bam': bias_variable([n_middle_character], 'actionNet_bias_middle'),\n",
    "    'baf': bias_variable([n_final_character], 'actionNet_bias_final'),\n",
    "    # 'bai': tf.get_variable('actionNet_bias_initial', [n_initial_character], tf.float32),\n",
    "    # 'bam': tf.get_variable('actionNet_bias_middle', [n_middle_character], tf.float32),\n",
    "    # 'baf': tf.get_variable('actionNet_bias_final', [n_final_character], tf.float32),\n",
    "    # for glimpse network\n",
    "    'bg1': bias_variable([8], 'glimpseNet_bias_conv1'),\n",
    "    'bg2': bias_variable([3], 'glimpseNet_bias_conv2'),\n",
    "    # 'bg3': bias_variable([3], 'glimpseNet_bias_conv3'),\n",
    "    'bg_fc': bias_variable([lstm_size], 'glimpseNet_bias_fc'),\n",
    "    'bg_lh': bias_variable([lstm_size], 'glimpseNet_bias_loc2hidden'),\n",
    "    'bg_glh_gf': bias_variable([lstm_size], 'glimpseNet_bias_feature'),\n",
    "\n",
    "    # 'bg1': tf.get_variable('glimpseNet_bias_conv1', [16], tf.float32),\n",
    "    # 'bg2': tf.get_variable('glimpseNet_bias_conv2', [64], tf.float32),\n",
    "    # 'bg3': tf.get_variable('glimpseNet_bias_conv3', [3], tf.float32),\n",
    "    # 'bg_fc': tf.get_variable('glimpseNet_bias_fc', [lstm_size], tf.float32),\n",
    "    # 'bg_lh': tf.get_variable('glimpseNet_bias_loc2hidden', [lstm_size], tf.float32),\n",
    "    # 'bg_glh_gf': tf.get_variable('glimpseNet_bias_feature', [lstm_size], tf.float32),\n",
    "    # for core network\n",
    "    'bo': bias_variable([lstm_size], 'coreNet_bias_out'),\n",
    "    # 'bo': tf.get_variable('coreNet_bias_out', [lstm_size], tf.float32)\n",
    "}\n",
    "\n",
    "# Model Build\n",
    "print(' Building model..')\n",
    "outputs, mean_locs, sampled_locs, baselines, actions, action_logits, predicted_labels = model2(x, w, b)\n",
    "\n",
    "# Loss Fuction\n",
    "print(' Defining loss function..')\n",
    "logllratios, cross_entropies, baseline_mse, total_loss, grads, var_list, accs = losses2(\n",
    "    actions=actions,\n",
    "    action_logits=action_logits,\n",
    "    mean_locs=mean_locs,\n",
    "    sampled_locs=sampled_locs,\n",
    "    baselines=baselines,\n",
    "    labels=Y)\n",
    "\n",
    "acc_element = [tf.reduce_mean(acc) for acc in accs]\n",
    "acc_total = tf.reduce_mean(tf.stack(acc_element))\n",
    "\n",
    "# Reconstruction Building.. (for pretraining)\n",
    "if pretrain_flag:\n",
    "    c_recon, c_recon_cost, g_recon, g_recon_cost, total_recon_cost, train_op_r = pretrain_losses(x, w, b)\n",
    "\n",
    "# Optimizer\n",
    "print(' Creating optimizer..')\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "# train_op = optimizer.apply_gradients(zip(grads, var_list))\n",
    "train_op = optimizer.minimize(total_loss)\n",
    "\n",
    "# tensorboard visualization for the parameters\n",
    "print(' Summarizing tensor variables and scalar to visualize by tensorboard..')\n",
    "# for context network\n",
    "variable_summaries(w['wc1'], \"contextNet_weight_conv1\")\n",
    "variable_summaries(b['bc1'], \"contextNet_bias_conv1\")\n",
    "variable_summaries(w['wc2'], \"contextNet_weight_conv2\")\n",
    "variable_summaries(b['bc2'], \"contextNet_bias_conv2\")\n",
    "# variable_summaries(w['wc3'], \"contextNet_weight_conv3\")\n",
    "# variable_summaries(b['bc3'], \"contextNet_bias_conv3\")\n",
    "# for emission network\n",
    "variable_summaries(w['we_bl'], \"emissionNet_weight_baseline\")\n",
    "variable_summaries(b['be_bl'], \"emissionNet_bias_baseline\")\n",
    "variable_summaries(w['we_h_nl'], \"emissionNet_weight_hidden_normalize_loc\")\n",
    "variable_summaries(b['be_h_nl'], \"emissionNet_bias_hidden_normalize_loc\")\n",
    "# for action network\n",
    "variable_summaries(w['wai'], \"actionNet_weight_initial\")\n",
    "variable_summaries(b['bai'], \"actionNet_bias_initial\")\n",
    "variable_summaries(w['wam'], \"actionNet_weight_middle\")\n",
    "variable_summaries(b['bam'], \"actionNet_bias_middle\")\n",
    "variable_summaries(w['waf'], \"actionNet_weight_final\")\n",
    "variable_summaries(b['baf'], \"actionNet_bias_final\")\n",
    "# for glimpse network\n",
    "variable_summaries(w['wg1'], \"glimpseNet_weight_conv1\")\n",
    "variable_summaries(b['bg1'], \"glimpseNet_bias_conv1\")\n",
    "variable_summaries(w['wg2'], \"glimpseNet_weight_conv2\")\n",
    "variable_summaries(b['bg2'], \"glimpseNet_bias_conv2\")\n",
    "# variable_summaries(w['wg3'], \"glimpseNet_weight_conv3\")\n",
    "# variable_summaries(b['bg3'], \"glimpseNet_bias_conv3\")\n",
    "variable_summaries(w['wg_fc'], \"glimpseNet_weight_fc\")\n",
    "variable_summaries(b['bg_fc'], \"glimpseNet_bias_fc\")\n",
    "\n",
    "variable_summaries(w['wg_lh'], \"glimpseNet_weight_loc2hidden\")\n",
    "variable_summaries(b['bg_lh'], \"glimpseNet_bias_loc2hidden\")\n",
    "variable_summaries(w['waf'], \"actionNet_weight_final\")\n",
    "variable_summaries(b['baf'], \"actionNet_bias_final\")\n",
    "\n",
    "variable_summaries(w['wg_gh_gf'], \"glimpseNet_weight_glimpse2feature\")\n",
    "variable_summaries(w['wg_lh_gf'], \"glimpseNet_weight_locHidden2feature\")\n",
    "variable_summaries(b['bg_glh_gf'], \"glimpseNet_bias_feature\")\n",
    "# for core network\n",
    "variable_summaries(w['wo'], \"coreNet_weight_out\")\n",
    "variable_summaries(b['bo'], \"coreNet_bias_out\")\n",
    "\n",
    "# tensorboard visualization for the performance metrics\n",
    "tf.summary.scalar(\"Loss/loglikelihood_ratio\", logllratios)\n",
    "tf.summary.scalar(\"Loss/baseline_mse\", baseline_mse)\n",
    "tf.summary.scalar(\"Loss/cross_entropies\", cross_entropies)\n",
    "for i, acc_arg in enumerate(acc_element):\n",
    "    tf.summary.scalar(\"Accuracy/accuracy_%d_element\" % (i+1), acc_arg)\n",
    "# tf.summary.scalar(\"Accuracy/accuracy_initial\", acc_i)\n",
    "# tf.summary.scalar(\"Accuracy/accuracy_middle\", acc_m)\n",
    "# tf.summary.scalar(\"Accuracy/accuracy_final\", acc_f)\n",
    "tf.summary.scalar(\"Accuracy/accuracy_total\", acc_total)\n",
    "tf.summary.scalar(\"Loss/total_loss\", total_loss)\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "############################################################### Training #####################\n",
    "# Model Training\n",
    "\n",
    "# Data set : PHD08\n",
    "print(' Loading datasets..')\n",
    "dataset = DataSet(args)\n",
    "\n",
    "# Session initialize\n",
    "print(' Initializing session..')\n",
    "sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "sess_config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=sess_config)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 28, 28, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_images, next_labels = dataset.train_data.next_batch(batch_size)\n",
    "if next_images.shape[0] != batch_size:\n",
    "    next_images, next_labels = dataset.train_data.next_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Start..!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (16, 64, 64) for Tensor 'image:0', which has shape '(16, 64, 64, 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ee1e69df5fdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                mean_locs, predicted_labels]\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_ent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py36tf1x/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py36tf1x/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1097\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (16, 64, 64) for Tensor 'image:0', which has shape '(16, 64, 64, 1)'"
     ]
    }
   ],
   "source": [
    "# Pre-Training context, glimpse net using reconstruction error\n",
    "if pretrain_flag:\n",
    "    pretrain(c_recon, c_recon_cost, g_recon, g_recon_cost, total_recon_cost, train_op_r, pretrain_step)\n",
    "\n",
    "\n",
    "print(' Training Start..!')\n",
    "for step in range(total_step):\n",
    "    start_time = time.time()\n",
    "    next_images, next_labels = dataset.train_data.next_batch(batch_size)\n",
    "    if next_images.shape[0] != batch_size:\n",
    "        next_images, next_labels = dataset.train_data.next_batch(batch_size)\n",
    "\n",
    "    if next_images[0].shape[0] != img_sz or next_images[0].shape[1] != img_sz:\n",
    "        next_images = np.array([cv2.resize(img, (img_sz, img_sz)) for img in next_images])\n",
    "        next_images = np.expand_dims(next_images, -1)\n",
    "\n",
    "    feed_dict = {x: next_images, Y: next_labels}\n",
    "\n",
    "    fetches = [train_op, total_loss, baseline_mse, cross_entropies, acc_element, acc_total,\n",
    "               mean_locs, predicted_labels]\n",
    "\n",
    "    _, t_loss, b_mse, x_ent, acc_list, t_acc, m_locs, p_labels = sess.run(fetches, feed_dict=feed_dict)\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    if step % 50 == 0:\n",
    "        print('Step %d: total_loss = %.5f, t_acc = %.3f (%.3f sec) mse = %.5f, x_ent = %.5f' % (step, t_loss,\n",
    "                                                                                                t_acc, duration,\n",
    "                                                                                                b_mse, x_ent*0.1))\n",
    "\n",
    "        summary_str = sess.run(summary_op, feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "        if step > 0 and step % 10000 == 0:\n",
    "            saver.save(sess, os.path.join(save_path, str(step) + \".ckpt\"))\n",
    "            evaluate(dataset, sess, [x, Y, acc_total] + acc_element)\n",
    "            mls = np.array(m_locs)\n",
    "\n",
    "            print('   mean locations: ', mls[:,0,:])\n",
    "        if step % 500 == 0 and n_element_per_character == 3:\n",
    "            for i in range(min(batch_size, 3)):\n",
    "                predicted_label = p_labels[:, i]\n",
    "                true_label = next_labels[i]\n",
    "\n",
    "                label_str = \"   >>> True Label : [\" \\\n",
    "                            + dataset.i2c_i[true_label[0]] \\\n",
    "                            + dataset.i2c_m[true_label[1]] \\\n",
    "                            + dataset.i2c_f[true_label[2]] \\\n",
    "                            + \"]   Predicted Label: [\" \\\n",
    "                            + dataset.i2c_i[predicted_label[0]] \\\n",
    "                            + dataset.i2c_m[predicted_label[1]] \\\n",
    "                            + dataset.i2c_f[predicted_label[2]] + \"]\"\n",
    "                print(label_str)\n",
    "\n",
    "        if step % 1000 == 0 and draw:\n",
    "            def visualize_glimpse_movement(image, locs):\n",
    "                r_image = cv2.resize(image, (image.shape[1] * 15, image.shape[0] * 15))\n",
    "                r_image = np.expand_dims(r_image, -1)\n",
    "\n",
    "                rows = r_image.shape[0]\n",
    "                cols = r_image.shape[1]\n",
    "                n_channel = r_image.shape[2]\n",
    "                disp = r_image.copy()\n",
    "                if n_channel == 1:\n",
    "                    disp = cv2.cvtColor(disp, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "                pts = []\n",
    "                for loc in locs:\n",
    "                    x = int((loc[0] + 1) * 0.5 * cols + 0.5)\n",
    "                    y = int((loc[1] + 1) * 0.5 * rows + 0.5)\n",
    "                    pts.append((x, y))\n",
    "\n",
    "                    cv2.circle(disp, (x, y), 1, (0, 255, 0), 3)\n",
    "\n",
    "                start_color = 100\n",
    "                color_gap = (255 - start_color) // (len(pts) - 1)\n",
    "                for i in range(len(pts) - 1):\n",
    "                    color = min(255, start_color + i * color_gap)\n",
    "                    cv2.line(disp, pts[i], pts[i+1], (0, color, 0), 2)\n",
    "                    cv2.line(disp, pts[i], pts[i + 1], (0, 255, 0), 2)\n",
    "                    cv2.circle(disp, pts[i], 4, (0, 255, 0), 3)\n",
    "\n",
    "                cv2.circle(disp, pts[0], 4, (255, 0, 0), 4)\n",
    "                cv2.circle(disp, pts[-1], 4, (0, 0, 255), 4)\n",
    "                return disp\n",
    "\n",
    "            m_locs = np.array(m_locs)\n",
    "            m_locs = np.transpose(m_locs, [1, 0, 2])\n",
    "            disp_list = []\n",
    "            for i in range(min(batch_size, 3)):\n",
    "                disp = visualize_glimpse_movement(next_images[i], m_locs[i])\n",
    "                cv2.imwrite(os.path.join(image_log_path, 'glimpse_%d_(%d).png' % (step, i)), disp)\n",
    "\n",
    "sess.close()\n",
    "print(' Training has been finished..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36tf1x",
   "language": "python",
   "name": "py36tf1x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
